# configuration to run l1-regularized logistic regression on the ctr dataset
app_name: "ctr"
parameter_name: "ctr_w"
linear_method {
training_data {
format: TEXT
text: ADFEA
file: "../../data/ctrc/train/part-000[0-1].*"
# format: PROTO
# file: "../../data/ctrb/train/part.*"
}

# validation_data {
# format: TEXT
# text: ADFEA
# file: "../../data/ctrc/test/part-000[0-7].*"
# }

# each worker dumps its processed data into local disk (or hdfs). it will
# accelerate the data loading if training on the same data again. but remember
# to delete all caches if (1) # of workers is changed; (2) data are changed.
# local_cache {
# format: BIN
# # file: "/l0/ctrc"
# file: "../cache/ctrc/"
# }

# model_output {
# format: TEXT
# file: "../output/ctr"
# }

loss {
type: LOGIT
}

# lambda * \| w \|_1
penalty {
type: L1
lambda: 5
}

learning_rate {
type: CONSTANT
eta: 1
}

solver {
# turn it off only for debug use
random_feature_block_order : true
# block-coordinate updating. we divide a feature group into feature_block_ratio
# x nnz_feature_per_instance blocks. if = 0, then use all features
feature_block_ratio : 4
# max # of iterations
max_pass_of_data : 20
# bounded-delay consistency
max_block_delay: 8
# convergance critiria. stop if the relative objective <= epsilon
epsilon : 2e-5
# important feature groups, update them eailer
prior_fea_group: 127            # the bias feature (all one)
prior_fea_group: 120            # the position rank feature
# features which occurs <= *tail_feature_count* will be filtered before training
tail_feature_freq: 4
}

darling {
# trust region search
delta_init_value : 1
delta_max_value : 5
# increasing this number will reduce the effect of kkt filter. a very large
# number, such as 1e20 will turn off the kkt filter
kkt_filter_threshold_ratio : 10
}
}
