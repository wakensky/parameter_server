# configuration to run l1-regularized logistic regression on the ctr dataset
app_name: "ctr"
parameter_name: "ctr_w"
linear_method {
training_data {
format: TEXT
text: TERAFEA
file: "/ss_ml/recommend/train_data/batch/terafea/201410150000-201410160000/instance/part-.*"

hdfs {
home: "/opt/tiger/yarn_deploy/hadoop-2.3.0-cdh5.1.0/"
}
# file: "../../data/ctrc/train/part-000[0-1].*"
# format: PROTO
# file: "../../data/ctrb/train/part.*"
}

# validation_data {
# format: TEXT
# text: ADFEA
# file: "../../data/ctrc/test/part-000[0-7].*"
# }

# each worker dumps its processed data into local disk (or hdfs). it will
# accelerate the data loading if training on the same data again. but remember
# to delete all caches if (1) # of workers is changed; (2) data are changed.
local_cache {
format: BIN
file: "/data00/tiger/ps_cdn/training_data/"
# file: "/l0/ctrc"
# file: "../cache/ctrc/"
}

# model_output {
# format: TEXT
# file: "../output/ctr"
# }

loss {
type: LOGIT
}

# lambda * \| w \|_1
penalty {
type: L1
lambda: 5
}

learning_rate {
type: CONSTANT
eta: 1
}

solver {
# turn it off only for debug use
random_feature_block_order : true
# block-coordinate updating. we divide a feature group into feature_block_ratio
# x nnz_feature_per_instance blocks. if = 0, then use all features
feature_block_ratio : 4
# max # of iterations
max_pass_of_data : 20
# bounded-delay consistency
max_block_delay: 8
# convergance critiria. stop if the relative objective <= epsilon
epsilon : 2e-5
# important feature groups, update them eailer
prior_fea_group: 127            # the bias feature (all one)
prior_fea_group: 120            # the position rank feature
# features which occurs <= *tail_feature_count* will be filtered before training
tail_feature_freq: 4

# It controls the countmin size. We filter the tail features by countmin, which
# is more efficient than hash, but still is the memory bottleneck for servers. A
# smaller ratio reduces the memory footprint, but may increase the size of
# filtered feature.
countmin_n_ratio: .66

# In preprocessing, feature group is processed one by one. It is the main memory
# bottleneck for workers. This number control how many feature groups can be in
# memory at the same time. A smaller number reduce the workers' memory
# footprint, but may slow down the preprocessing speed.
max_num_parallel_groups_in_preprocessing: 100
}

darling {
# trust region search
delta_init_value : 1
delta_max_value : 5
# increasing this number will reduce the effect of kkt filter. a very large
# number, such as 1e20 will turn off the kkt filter
kkt_filter_threshold_ratio : 10
}
}
